{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mrna_content_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the optimal model for mRNA content regression using Optuna\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load common.py\n",
    "# export\n",
    "\n",
    "import pandas\n",
    "import os\n",
    "import numpy\n",
    "import seaborn\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ehv.core' from '/home/maximl/Data/dev/active/EhV-analysis/ehv/core.py'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import time\n",
    "import fcsparser\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "from calibration_singlecell import classification\n",
    "from functools import partial\n",
    "from ehv import mrna_content_regression, core\n",
    "import scipy.stats\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn import metrics\n",
    "from joblib import load, dump\n",
    "import sklearn.neural_network\n",
    "import optuna\n",
    "import click\n",
    "import xgboost\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "reload(core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.load_config(\"../config_cn1346.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, df = fcsparser.parse(core.FCS_NONINTENSITY)\n",
    "df_meta = df[[\"label\", \"replicate\", \"timepoint\"]].astype(int)\n",
    "df_meta[\"timepoint\"] = df_meta[\"timepoint\"].map(lambda a: meta[\"timepoint\"].split(\",\")[a])\n",
    "df_meta[\"replicate\"] = df_meta[\"replicate\"].map(lambda a: meta[\"replicate\"].split(\",\")[a])\n",
    "df = df.drop(columns=[\"label\", \"replicate\", \"timepoint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fluor_df = fcsparser.parse(core.FCS_FLUOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_funcs = [\n",
    "    metrics.explained_variance_score,\n",
    "    metrics.max_error,\n",
    "    metrics.mean_absolute_error,\n",
    "    metrics.mean_squared_error,\n",
    "    metrics.mean_squared_error,\n",
    "    metrics.median_absolute_error,\n",
    "    metrics.r2_score\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BF regression on targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_df = df.filter(regex=\"(i?).*BF.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_fold, inner_folds = load(core.FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def standardizer(X, mean, std=1.0):\n",
    "    return (X-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def preprocess_funcs(df, target, outer_fold, inner_folds):\n",
    "\n",
    "    outer_fold_funcs = []\n",
    "    inner_fold_funcs = []\n",
    "\n",
    "    for i, (train_idx, _) in enumerate(PredefinedSplit(outer_fold).split()):\n",
    "\n",
    "        X = df.iloc[train_idx]\n",
    "        y = target.iloc[train_idx]\n",
    "\n",
    "        outer_fold_funcs.append((\n",
    "            partial(standardizer, mean=X.mean(), std=X.std()),\n",
    "            partial(standardizer, mean=y.mean())\n",
    "        ))\n",
    "\n",
    "        for inner_fold in inner_folds[i]:\n",
    "            tmp = []\n",
    "            for nested_train_idx, _ in PredefinedSplit(inner_fold).split():\n",
    "\n",
    "                X_train = X.iloc[nested_train_idx]\n",
    "                mean, std = X.mean(), X.std()\n",
    "\n",
    "                y_mean = y.iloc[nested_train_idx].mean()\n",
    "\n",
    "                tmp.append((\n",
    "                    partial(standardizer, mean=mean, std=std),\n",
    "                    partial(standardizer, mean=y_mean)\n",
    "                ))\n",
    "\n",
    "            inner_fold_funcs.append(tmp)\n",
    "            \n",
    "    return outer_fold_funcs, inner_fold_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_tmr_funcs = preprocess_funcs(bf_df, fluor_df[\"IntensityMCTMRlogicle\"], outer_fold, inner_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def define_dummy(trial):\n",
    "    return sklearn.dummy.DummyRegressor()\n",
    "\n",
    "def define_mlp(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 10)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f\"hidden_layer_size_{i}\", 10, 100))\n",
    "\n",
    "    return sklearn.neural_network.MLPRegressor(\n",
    "        hidden_layer_sizes = layers,\n",
    "        activation = trial.suggest_categorical(\"activation\", ['identity', 'logistic', 'tanh', 'relu']),\n",
    "        learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-1, log=True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def define_xgb(trial):\n",
    "    return xgboost.XGBRegressor(\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 400),\n",
    "        tree_method = \"gpu_hist\",\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        subsample = trial.suggest_float('subsample', 0.1, 1.0)\n",
    "    )\n",
    "    \n",
    "    \n",
    "def define_sgd(trial):\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [\"constant\", \"optimal\"])\n",
    "    if learning_rate == \"constant\":\n",
    "        eta0 = trial.suggest_float('eta0', 1e-4, 1e-1, log=True)\n",
    "    else:\n",
    "        eta0 = 0.01 # default\n",
    "\n",
    "    return sklearn.linear_model.SGDRegressor(\n",
    "        loss = trial.suggest_categorical(\"loss\", ['squared_loss', 'huber']),\n",
    "        fit_intercept = False,\n",
    "        learning_rate = learning_rate,\n",
    "        eta0 = trial.suggest_float('eta0', 1e-4, 1e-1, log=True),\n",
    "        alpha = trial.suggest_float(\"alpha\", 0, 1)\n",
    "    )\n",
    "\n",
    "definition_dict = {\n",
    "    \"mlp\": define_mlp,\n",
    "    \"sgd\": define_sgd,\n",
    "    \"xgb\": define_xgb,\n",
    "    \"dummy\": define_dummy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def objective(trial, inner_fold, funcs, X, y, model_names):\n",
    "    \n",
    "    logging.getLogger(__name__).info(model_names)\n",
    "    \n",
    "    model_name = trial.suggest_categorical(\"model_name\", model_names)\n",
    "    model = definition_dict[model_name](trial)\n",
    "    \n",
    "    scores = []\n",
    "    for i, (func, (train_idx, val_idx)) in enumerate(zip(funcs, PredefinedSplit(inner_fold).split())):\n",
    "        logging.getLogger(__name__).info(f\"Fitting model on inner fold {i}\")\n",
    "        \n",
    "        X_train = func[0](X.iloc[train_idx])\n",
    "        y_train = func[1](y.iloc[train_idx])\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        X_val = func[0](X.iloc[val_idx])\n",
    "        y_val = func[1](y.iloc[val_idx])\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        \n",
    "        scores.append(sklearn.metrics.mean_squared_error(y_val, y_val_pred))\n",
    "        \n",
    "    return numpy.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def nested_cross_validation_with_hyperparam_optimization(\n",
    "        df:pandas.DataFrame, target:pandas.Series, model_names:str,\n",
    "        optuna_storage:str, optuna_n_trials:int, optuna_study_name_fmt:str, optuna_n_jobs:int\n",
    "    ):\n",
    "    \n",
    "    outer_fold, inner_folds = load(core.FOLDS)\n",
    "    outer_fold_funcs, inner_folds_funcs = preprocess_funcs(df, target, outer_fold, inner_folds)\n",
    "    \n",
    "    scores = []\n",
    "    for i, ((train_idx, test_idx), outer_func) in enumerate(zip(PredefinedSplit(outer_fold).split(), outer_fold_funcs)):\n",
    "\n",
    "        X = df.iloc[train_idx]\n",
    "        y = target.iloc[train_idx]\n",
    "\n",
    "        scores_inner = []\n",
    "        for j, (inner_fold, funcs) in enumerate(zip(inner_folds[i], inner_folds_funcs)):\n",
    "\n",
    "            logging.getLogger(__name__).info(f\"Hyper param tuning on outer fold {i}, inner fold repeat {j+1}/{len(inner_folds)}\")\n",
    "\n",
    "            fold_objective = partial(objective, inner_fold=inner_fold, funcs=funcs, X=X, y=y, model_names=model_names)\n",
    "\n",
    "            # select best parameters for model using Optuna\n",
    "            study = optuna.create_study(study_name=optuna_study_name_fmt % (i, j), direction='minimize', storage=optuna_storage, load_if_exists=True)\n",
    "            study.set_user_attr(\"cross-validation splits\", core.FOLDS)\n",
    "            study.optimize(fold_objective, n_trials=optuna_n_trials, n_jobs=optuna_n_jobs)\n",
    "\n",
    "            logging.getLogger(__name__).info(f\"Retraining on outer fold {i}, inner fold repeat {j+1}/{len(inner_folds)}\")\n",
    "\n",
    "            # retrain model with best parameters on train+val data\n",
    "            fixed_trial = optuna.trial.FixedTrial(study.best_params)\n",
    "            model = definition_dict[fixed_trial.params[\"model_name\"]](fixed_trial)\n",
    "            \n",
    "            model.fit(outer_func[0](X), outer_func[1](y))\n",
    "\n",
    "            # predict X_test with retrained model\n",
    "            X_test = outer_func[0](df.iloc[test_idx])\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            # evaluate predictions\n",
    "            y_test = outer_func[1](target.iloc[test_idx])\n",
    "\n",
    "            scores_inner.append(sklearn.metrics.mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "        scores.append(scores_inner)\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Hyper param tuning on outer fold 0, inner fold repeat 1/3\n",
      "[I 2020-08-26 18:38:11,336] Using an existing study with name 'test_run_2_0_0' instead of creating a new one.\n",
      "INFO:__main__:['xgb']\n",
      "INFO:__main__:Fitting model on inner fold 0\n",
      "INFO:__main__:Fitting model on inner fold 1\n",
      "INFO:__main__:Fitting model on inner fold 2\n",
      "[I 2020-08-26 18:39:03,598] Trial 1 finished with value: 0.1596449762582779 and parameters: {'model_name': 'xgb', 'n_estimators': 237, 'max_depth': 9, 'learning_rate': 0.001391080006585426, 'subsample': 0.5616181033132915}. Best is trial 1 with value: 0.1596449762582779.\n",
      "INFO:__main__:['xgb']\n",
      "INFO:__main__:Fitting model on inner fold 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-dc3abfbc686e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m scores = nested_cross_validation_with_hyperparam_optimization(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbf_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfluor_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IntensityMCTMRlogicle\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"xgb\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sqlite:///tmp/test.sqlite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_run_2_%d_%d\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-48-e68542f5e1e0>\u001b[0m in \u001b[0;36mnested_cross_validation_with_hyperparam_optimization\u001b[0;34m(df, target, model_names, optuna_storage, optuna_n_trials, optuna_study_name_fmt)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna_study_name_fmt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_user_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cross-validation splits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOLDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna_n_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Retraining on outer fold {i}, inner fold repeat {j+1}/{len(inner_folds)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 self._optimize_sequential(\n\u001b[0m\u001b[1;32m    292\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    652\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;31m# type: (...) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Trial {} pruned. {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-77c86cefa279>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, inner_fold, funcs, X, y, model_names)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    542\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eval_metric'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[1;32m    545\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    206\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/ml/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[1;32m   1248\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = nested_cross_validation_with_hyperparam_optimization(\n",
    "    bf_df, fluor_df[\"IntensityMCTMRlogicle\"],\n",
    "    [\"xgb\"], \"sqlite:///tmp/test.sqlite\", 2, \"test_run_2_%d_%d\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--target-col\", \"-t\", type=str, default=\"IntensityMCTMRlogicle\")\n",
    "@click.option(\"--storage-path\", \"-s\", type=click.Path(file_okay=False, exists=False), required=True, help=\"Provide absolute path.\")\n",
    "@click.option(\"--model\", \"-m\", type=click.Choice(definition_dict.keys()), multiple=True, default=definition_dict.keys())\n",
    "@click.option(\"--config\", \"-c\", type=click.Path(dir_okay=False, exists=True), default=\"../config_cn1346.yml\")\n",
    "@click.option(\"--optuna-n-trials\", \"-ot\", type=click.IntRange(0), default=5)\n",
    "@click.option(\"--optuna-study-name\", \"-on\", type=str, default=5)\n",
    "@click.option(\"--optuna-n-jobs\", \"-oj\", type=int, default=-1)\n",
    "@click.option(\"--overwrite\", \"-w\", is_flag=True, default=False)\n",
    "@click.option(\"--verbose\", \"-v\", is_flag=True, default=False)\n",
    "def nested_cross_validation_with_hyperparam_optimization_command(target_col, storage_path, model, config, optuna_n_trials, optuna_study_name, optuna_n_jobs, overwrite, verbose):\n",
    "    \n",
    "    if optuna_n_jobs == -1:\n",
    "        optuna_n_jobs = cpu_count()\n",
    "    \n",
    "    core.load_config(config)\n",
    "    \n",
    "    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n",
    "    \n",
    "    meta, df = fcsparser.parse(core.FCS_NONINTENSITY)\n",
    "    df_meta = df[[\"label\", \"replicate\", \"timepoint\"]].astype(int)\n",
    "    df_meta[\"timepoint\"] = df_meta[\"timepoint\"].map(lambda a: meta[\"timepoint\"].split(\",\")[a])\n",
    "    df_meta[\"replicate\"] = df_meta[\"replicate\"].map(lambda a: meta[\"replicate\"].split(\",\")[a])\n",
    "    df = df.drop(columns=[\"label\", \"replicate\", \"timepoint\"])\n",
    "    \n",
    "    _, fluor_df = fcsparser.parse(core.FCS_FLUOR)\n",
    "    \n",
    "    df = bf_df = df.filter(regex=\"(i?).*BF.*\")\n",
    "    target = fluor_df[target_col]\n",
    "    \n",
    "    optuna_storage = \"sqlite:////\"+os.path.join(storage_path, \"optuna_db.sqlite\")\n",
    "    \n",
    "    if overwrite and os.path.isfile(optuna_storage):\n",
    "        os.remove(optuna_storage)\n",
    "        \n",
    "    optuna_study_name_fmt = optuna_study_name + \"_%d_%d\"\n",
    "    \n",
    "    scores = nested_cross_validation_with_hyperparam_optimization(df, target, model, optuna_storage, optuna_n_trials, optuna_study_name_fmt, optuna_n_jobs)\n",
    "    \n",
    "    dump(scores, os.path.join(storage_path, \"scores.dat\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
