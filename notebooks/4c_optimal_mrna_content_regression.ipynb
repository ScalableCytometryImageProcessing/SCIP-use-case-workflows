{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp mrna_content_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the optimal model for mRNA content regression using Optuna\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load common.py\n",
    "# export\n",
    "\n",
    "import pandas\n",
    "import os\n",
    "import numpy\n",
    "import seaborn\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "from ehv import core\n",
    "\n",
    "numpy.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ehv.splits' from '/home/maximl/Data/dev/active/EhV-analysis/ehv/splits.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "\n",
    "import time\n",
    "import fcsparser\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "from functools import partial\n",
    "from ehv import mrna_content_regression, splits\n",
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "from joblib import load, dump\n",
    "import sklearn.neural_network\n",
    "import optuna\n",
    "import click\n",
    "import xgboost\n",
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "reload(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "core.load_config(\"../config_cn1346.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, df = fcsparser.parse(core.FCS_NONINTENSITY)\n",
    "df_meta = df[[\"label\", \"replicate\", \"timepoint\"]].astype(int)\n",
    "df_meta[\"timepoint\"] = df_meta[\"timepoint\"].map(lambda a: meta[\"timepoint\"].split(\",\")[a])\n",
    "df_meta[\"replicate\"] = df_meta[\"replicate\"].map(lambda a: meta[\"replicate\"].split(\",\")[a])\n",
    "df = df.drop(columns=[\"label\", \"replicate\", \"timepoint\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, fluor_df = fcsparser.parse(core.FCS_FLUOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_funcs = [\n",
    "    metrics.explained_variance_score,\n",
    "    metrics.max_error,\n",
    "    metrics.mean_absolute_error,\n",
    "    metrics.mean_squared_error,\n",
    "    metrics.mean_squared_error,\n",
    "    metrics.median_absolute_error,\n",
    "    metrics.r2_score\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BF regression on targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_df = df.filter(regex=\"(i?).*BF.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_fold, inner_folds = load(core.FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_tmr_funcs = splits.preprocess_funcs(bf_df, fluor_df[\"IntensityMCTMRlogicle\"], outer_fold, inner_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def define_dummy(trial):\n",
    "    return sklearn.dummy.DummyRegressor()\n",
    "\n",
    "def define_mlp(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 2, 10)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        layers.append(trial.suggest_int(f\"hidden_layer_size_{i}\", 10, 100))\n",
    "\n",
    "    return sklearn.neural_network.MLPRegressor(\n",
    "        hidden_layer_sizes = layers,\n",
    "        activation = trial.suggest_categorical(\"activation\", ['identity', 'logistic', 'tanh', 'relu']),\n",
    "        learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-1, log=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def define_xgb(trial):\n",
    "    return xgboost.XGBRegressor(\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 400),\n",
    "        tree_method = \"gpu_hist\",\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20),\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),\n",
    "        subsample = trial.suggest_float('subsample', 0.1, 1.0)\n",
    "    )\n",
    "\n",
    "\n",
    "def define_sgd(trial):\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [\"constant\", \"optimal\"])\n",
    "    if learning_rate == \"constant\":\n",
    "        eta0 = trial.suggest_float('eta0', 1e-4, 1e-1, log=True)\n",
    "    else:\n",
    "        eta0 = 0.01 # default\n",
    "\n",
    "    return sklearn.linear_model.SGDRegressor(\n",
    "        loss = trial.suggest_categorical(\"loss\", ['squared_loss', 'huber']),\n",
    "        fit_intercept = False,\n",
    "        learning_rate = learning_rate,\n",
    "        eta0 = trial.suggest_float('eta0', 1e-4, 1e-1, log=True),\n",
    "        alpha = trial.suggest_float(\"alpha\", 0, 1)\n",
    "    )\n",
    "\n",
    "definition_dict = {\n",
    "    \"mlp\": define_mlp,\n",
    "    \"sgd\": define_sgd,\n",
    "    \"xgb\": define_xgb,\n",
    "    \"dummy\": define_dummy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def objective(trial, inner_fold, funcs, X, y, model_names):\n",
    "\n",
    "    logging.getLogger(__name__).info(model_names)\n",
    "\n",
    "    model_name = trial.suggest_categorical(\"model_name\", model_names)\n",
    "    model = definition_dict[model_name](trial)\n",
    "\n",
    "    scores = []\n",
    "    for i, (func, (train_idx, val_idx)) in enumerate(zip(funcs, sklearn.model_selection.PredefinedSplit(inner_fold).split())):\n",
    "        logging.getLogger(__name__).info(f\"Fitting model on inner fold {i}\")\n",
    "\n",
    "        X_train = func[0](X.iloc[train_idx])\n",
    "        y_train = func[1](y.iloc[train_idx])\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        X_val = func[0](X.iloc[val_idx])\n",
    "        y_val = func[1](y.iloc[val_idx])\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        scores.append(sklearn.metrics.mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "    return numpy.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def run_optimization(i, j, X, y, X_test, y_test, inner_fold, inner_funcs, outer_func, model_names, optuna_storage, optuna_n_trials, optuna_study_name_fmt, optuna_n_jobs):\n",
    "    fold_objective = partial(objective, inner_fold=inner_fold, funcs=inner_funcs, X=X, y=y, model_names=model_names)\n",
    "\n",
    "    # select best parameters for model using Optuna\n",
    "\n",
    "    logging.getLogger(__name__).info(f\"(outer {i}, repeat {j}) Study started\")\n",
    "\n",
    "    study = optuna.create_study(study_name=optuna_study_name_fmt % (i, j), direction='minimize', storage=optuna_storage, load_if_exists=True)\n",
    "    study.set_user_attr(\"cross-validation splits\", core.FOLDS)\n",
    "\n",
    "    remaining_trials = optuna_n_trials\n",
    "    for t in study.trials:\n",
    "        if t.state == optuna.trial.TrialState.COMPLETE:\n",
    "            remaining_trials -= 1\n",
    "\n",
    "    if remaining_trials > 0:\n",
    "        logging.getLogger(__name__).info(f\"Running {remaining_trials} trials to reach {optuna_n_trials} completed trials.\")\n",
    "        study.optimize(fold_objective, n_trials=remaining_trials, n_jobs=optuna_n_jobs)\n",
    "\n",
    "    # retrain model with best parameters on train+val data\n",
    "\n",
    "    logging.getLogger(__name__).info(f\"(outer {i}, repeat {j}) Retraining model with best params\")\n",
    "\n",
    "    fixed_trial = optuna.trial.FixedTrial(study.best_params)\n",
    "    model_name = fixed_trial.suggest_categorical(\"model_name\", model_names)\n",
    "    model = definition_dict[model_name](fixed_trial)\n",
    "\n",
    "    model.fit(outer_func[0](X), outer_func[1](y))\n",
    "\n",
    "    # predict X_test with retrained model\n",
    "    y_test_pred = model.predict(outer_func[0](X_test))\n",
    "\n",
    "    mse = sklearn.metrics.mean_squared_error(outer_func[1](y_test), y_test_pred)\n",
    "    study.set_user_attr(\"mse_after_retrain\", float(mse))\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def nested_cross_validation_with_hyperparam_optimization(df:pandas.DataFrame, target:pandas.Series, n_procs:int, opt_func, **opt_kwargs):\n",
    "\n",
    "    outer_fold, inner_folds = load(core.FOLDS)\n",
    "    outer_fold_funcs, inner_folds_funcs = splits.preprocess_funcs(df, target, outer_fold, inner_folds)\n",
    "\n",
    "    with Pool(processes=n_procs) as pool:\n",
    "        # submit all optimization tasks to the pool\n",
    "        promises = []\n",
    "        for i, ((train_idx, test_idx), outer_func) in enumerate(zip(sklearn.model_selection.PredefinedSplit(outer_fold).split(), outer_fold_funcs)):\n",
    "\n",
    "            X = df.iloc[train_idx]\n",
    "            y = target.iloc[train_idx]\n",
    "            X_test = df.iloc[test_idx]\n",
    "            y_test = target.iloc[test_idx]\n",
    "\n",
    "            promises_inner = []\n",
    "            for j, (inner_fold, funcs) in enumerate(zip(inner_folds[i], inner_folds_funcs)):\n",
    "\n",
    "                # add func to pool\n",
    "                logging.getLogger(__name__).info(f\"Hyper param tuning on outer fold {i}, inner fold repeat {j+1}/{len(inner_folds)}\")\n",
    "                args=(i, j, X, y, X_test, y_test, inner_fold, funcs, outer_func)\n",
    "                promises_inner.append(pool.apply_async(opt_func, args, opt_kwargs))\n",
    "\n",
    "            promises.append(promises_inner)\n",
    "\n",
    "        # retrieve scores from pool once finished\n",
    "        scores = []\n",
    "        for promises_inner in promises:\n",
    "            scores_inner = []\n",
    "            for promise in promises_inner:\n",
    "                scores_inner.append(promise.get())\n",
    "            scores.append(scores_inner)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Hyper param tuning on outer fold 0, inner fold repeat 1/3\n",
      "INFO:__main__:Hyper param tuning on outer fold 0, inner fold repeat 2/3\n",
      "INFO:__main__:Hyper param tuning on outer fold 0, inner fold repeat 3/3\n",
      "INFO:__main__:Hyper param tuning on outer fold 1, inner fold repeat 1/3\n",
      "INFO:__main__:Hyper param tuning on outer fold 1, inner fold repeat 2/3\n",
      "INFO:__main__:Hyper param tuning on outer fold 1, inner fold repeat 3/3\n",
      "INFO:__main__:Hyper param tuning on outer fold 2, inner fold repeat 1/3\n",
      "INFO:__main__:Hyper param tuning on outer fold 2, inner fold repeat 2/3\n",
      "INFO:__main__:Hyper param tuning on outer fold 2, inner fold repeat 3/3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "In  \u001b[0;34m[17]\u001b[0m:\nLine \u001b[0;34m1\u001b[0m:     scores = nested_cross_validation_with_hyperparam_optimization(\n",
      "In  \u001b[0;34m[16]\u001b[0m:\nLine \u001b[0;34m33\u001b[0m:    scores_inner.append(promise.get())\n",
      "File \u001b[0;34m/home/maximl/.conda/envs/ml/lib/python3.8/multiprocessing/pool.py\u001b[0m, in \u001b[0;32mget\u001b[0m:\nLine \u001b[0;34m762\u001b[0m:   \u001b[36mself\u001b[39;49;00m.wait(timeout)\n",
      "File \u001b[0;34m/home/maximl/.conda/envs/ml/lib/python3.8/multiprocessing/pool.py\u001b[0m, in \u001b[0;32mwait\u001b[0m:\nLine \u001b[0;34m759\u001b[0m:   \u001b[36mself\u001b[39;49;00m._event.wait(timeout)\n",
      "File \u001b[0;34m/home/maximl/.conda/envs/ml/lib/python3.8/threading.py\u001b[0m, in \u001b[0;32mwait\u001b[0m:\nLine \u001b[0;34m558\u001b[0m:   signaled = \u001b[36mself\u001b[39;49;00m._cond.wait(timeout)\n",
      "File \u001b[0;34m/home/maximl/.conda/envs/ml/lib/python3.8/threading.py\u001b[0m, in \u001b[0;32mwait\u001b[0m:\nLine \u001b[0;34m302\u001b[0m:   waiter.acquire()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: \n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "scores = nested_cross_validation_with_hyperparam_optimization(\n",
    "    bf_df, fluor_df[\"IntensityMCTMRlogicle\"], 2, run_optimization,\n",
    "    model_names=[\"xgb\"], optuna_storage=\"sqlite:///tmp/test.sqlite\", optuna_n_trials=2, optuna_study_name_fmt=\"test_run_2_%d_%d\", optuna_n_jobs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "@click.command()\n",
    "@click.option(\"--target-col\", \"-t\", type=str, default=\"IntensityMCTMRlogicle\")\n",
    "@click.option(\"--storage-path\", \"-s\", type=click.Path(file_okay=False, exists=False), required=True, help=\"Provide absolute path.\")\n",
    "@click.option(\"--model\", \"-m\", type=click.Choice(definition_dict.keys()), multiple=True, default=definition_dict.keys())\n",
    "@click.option(\"--config\", \"-c\", type=click.Path(dir_okay=False, exists=True), default=\"../config_cn1346.yml\")\n",
    "@click.option(\"--optuna-n-trials\", \"-ot\", type=click.IntRange(0), default=5)\n",
    "@click.option(\"--optuna-study-name\", \"-on\", type=str, default=\"study\")\n",
    "@click.option(\"--optuna-n-jobs\", \"-oj\", type=int, default=-1)\n",
    "@click.option(\"--n-procs\", \"-p\", type=int, default=-1)\n",
    "@click.option(\"--overwrite\", \"-w\", is_flag=True, default=False)\n",
    "@click.option(\"--verbose\", \"-v\", is_flag=True, default=False)\n",
    "def nested_cross_validation_with_hyperparam_optimization_command(target_col, storage_path, model, config, optuna_n_trials, optuna_study_name, optuna_n_jobs, n_procs, overwrite, verbose):\n",
    "\n",
    "    if optuna_n_jobs == -1:\n",
    "        optuna_n_jobs = cpu_count()\n",
    "    if n_procs == -1:\n",
    "        n_procs = cpu_count()\n",
    "\n",
    "    core.load_config(config)\n",
    "\n",
    "    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n",
    "\n",
    "    meta, df = fcsparser.parse(core.FCS_NONINTENSITY)\n",
    "    df_meta = df[[\"label\", \"replicate\", \"timepoint\"]].astype(int)\n",
    "    df_meta[\"timepoint\"] = df_meta[\"timepoint\"].map(lambda a: meta[\"timepoint\"].split(\",\")[a])\n",
    "    df_meta[\"replicate\"] = df_meta[\"replicate\"].map(lambda a: meta[\"replicate\"].split(\",\")[a])\n",
    "    df = df.drop(columns=[\"label\", \"replicate\", \"timepoint\"])\n",
    "\n",
    "    _, fluor_df = fcsparser.parse(core.FCS_FLUOR)\n",
    "\n",
    "    df = bf_df = df.filter(regex=\"(i?).*BF.*\")\n",
    "    target = fluor_df[target_col]\n",
    "\n",
    "    optuna_storage = \"sqlite:////\"+os.path.join(storage_path, \"optuna_db.sqlite\")\n",
    "\n",
    "    if overwrite and os.path.isfile(optuna_storage):\n",
    "        os.remove(optuna_storage)\n",
    "\n",
    "    optuna_study_name_fmt = optuna_study_name + \"_%d_%d\"\n",
    "\n",
    "    scores = nested_cross_validation_with_hyperparam_optimization(\n",
    "        df, target, n_procs, run_optimization,\n",
    "        model_names=model, optuna_storage=optuna_storage,\n",
    "        optuna_n_trials=optuna_n_trials,\n",
    "        optuna_study_name_fmt=optuna_study_name_fmt, optuna_n_jobs=optuna_n_jobs)\n",
    "\n",
    "    dump(scores, os.path.join(storage_path, \"scores.dat\"))\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    nested_cross_validation_with_hyperparam_optimization_command()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
