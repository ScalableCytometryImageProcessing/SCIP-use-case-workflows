# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/4i_GMM_clustering_all_COI.ipynb (unless otherwise specified).

__all__ = ['func', 'n_components_search']

# Cell
# export

import pandas
import os
import numpy
import seaborn
import logging
import matplotlib.pyplot as plt
from matplotlib import cm
import matplotlib
from importlib import reload
from ehv import core
from joblib import load, dump
from pathlib import Path
import uuid
import re
import scipy

from ehv import load as e_load, core

plt.rcParams['figure.facecolor'] = 'white'

numpy.random.seed(42)

# Cell
from ehv import correlation, preprocessing_pipeline
from sklearn.mixture import GaussianMixture, BayesianGaussianMixture
from multiprocessing import Pool
from sklearn import metrics

# Cell

def func(X, i, seed):
    cluster_labels = BayesianGaussianMixture(n_components=i, covariance_type="diag", random_state=seed, max_iter=1000).fit_predict(X)
    return {
        "n_components": i,
        "seed": seed,
#         "silhouette": metrics.silhouette_score(X, cluster_labels, metric="euclidean"),
        "cal-har": metrics.calinski_harabasz_score(X, cluster_labels),
        "dav-boul": metrics.davies_bouldin_score(X, cluster_labels)
    }

def n_components_search(df, n_jobs, comp_range, repeats):
    data = []

    with Pool(processes=n_jobs) as pool:
        X = df.filter(regex="feat_pca")

        promises = []
        for n_comps in comp_range:
            numpy.random.seed(42)
            for rep in range(repeats):
                seed = 42 + numpy.random.randint(100)
                p = pool.apply_async(func, args=(X.copy(), n_comps, seed))
                promises.append(p)

        for i, p in enumerate(promises):
            data.append(p.get())
            print(i, end=" ")
        print()

    return data