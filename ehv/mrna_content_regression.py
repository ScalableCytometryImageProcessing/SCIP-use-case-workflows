# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/4c_optimal_mrna_content_regression.ipynb (unless otherwise specified).

__all__ = ['standardizer', 'preprocess_funcs', 'define_dummy', 'define_mlp', 'define_xgb', 'define_sgd',
           'definition_dict', 'objective', 'nested_cross_validation_with_hyperparam_optimization',
           'nested_cross_validation_with_hyperparam_optimization_command']

# Cell
# export

import pandas
import os
import numpy
import seaborn
import logging
import matplotlib.pyplot as plt
from importlib import reload

numpy.random.seed(42)

# Cell

import time
import fcsparser
import sklearn.linear_model
import sklearn.model_selection
from calibration_singlecell import classification
from functools import partial
from ehv import mrna_content_regression, core
import scipy.stats
from sklearn.model_selection import PredefinedSplit
from sklearn import metrics
from joblib import load, dump
import sklearn.neural_network
import optuna
import click
import xgboost

reload(core)

# Cell

def standardizer(X, mean, std=1.0):
    return (X-mean)/std

# Cell

def preprocess_funcs(df, target, outer_fold, inner_folds):

    outer_fold_funcs = []
    inner_fold_funcs = []

    for i, (train_idx, _) in enumerate(PredefinedSplit(outer_fold).split()):

        X = df.iloc[train_idx]
        y = target.iloc[train_idx]

        outer_fold_funcs.append((
            partial(standardizer, mean=X.mean(), std=X.std()),
            partial(standardizer, mean=y.mean())
        ))

        for inner_fold in inner_folds[i]:
            tmp = []
            for nested_train_idx, _ in PredefinedSplit(inner_fold).split():

                X_train = X.iloc[nested_train_idx]
                mean, std = X.mean(), X.std()

                y_mean = y.iloc[nested_train_idx].mean()

                tmp.append((
                    partial(standardizer, mean=mean, std=std),
                    partial(standardizer, mean=y_mean)
                ))

            inner_fold_funcs.append(tmp)

    return outer_fold_funcs, inner_fold_funcs

# Cell

def define_dummy(trial):
    return sklearn.dummy.DummyRegressor()

def define_mlp(trial):
    n_layers = trial.suggest_int("n_layers", 2, 10)
    layers = []
    for i in range(n_layers):
        layers.append(trial.suggest_int(f"hidden_layer_size_{i}", 10, 100))

    return sklearn.neural_network.MLPRegressor(
        hidden_layer_sizes = layers,
        activation = trial.suggest_categorical("activation", ['identity', 'logistic', 'tanh', 'relu']),
        learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-1, log=True)
    )


def define_xgb(trial):
    return xgboost.XGBRegressor(
        n_estimators = trial.suggest_int('n_estimators', 50, 400),
        tree_method = "gpu_hist",
        max_depth = trial.suggest_int("max_depth", 3, 20),
        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),
        subsample = trial.suggest_float('subsample', 0.1, 1.0)
    )


def define_sgd(trial):
    learning_rate = trial.suggest_categorical("learning_rate", ["constant", "optimal"])
    if learning_rate == "constant":
        eta0 = trial.suggest_float("eta0")
    else:
        eta0 = 0.01 # default

    return sklearn.linear_model.SGDRegressor(
        loss = trial.suggest_categorical("loss", ['squared_loss', 'huber']),
        fit_intercept = False,
        learning_rate = learning_rate,
        eta0 = trial.suggest_float('eta0', 1e-4, 1e-1, log=True),
        alpa = trial.suggest_float("alpha")
    )

definition_dict = {
    "mlp": define_mlp,
    "sgd": define_sgd,
    "xgb": define_xgb,
    "dummy": define_dummy
}

# Cell

def objective(trial, inner_fold, funcs, X, y, model_names):

    logging.getLogger(__name__).info(model_names)

    model_name = trial.suggest_categorical("model_name", model_names)
    model = definition_dict[model_name](trial)

    scores = []
    for i, (func, (train_idx, val_idx)) in enumerate(zip(funcs, PredefinedSplit(inner_fold).split())):
        logging.getLogger(__name__).info(f"Fitting model on inner fold {i}")

        X_train = func[0](X.iloc[train_idx])
        y_train = func[1](y.iloc[train_idx])

        model.fit(X_train, y_train)

        X_val = func[0](X.iloc[val_idx])
        y_val = func[1](y.iloc[val_idx])
        y_val_pred = model.predict(X_val)

        scores.append(sklearn.metrics.mean_squared_error(y_val, y_val_pred))

    return numpy.mean(scores)

# Cell

def nested_cross_validation_with_hyperparam_optimization(
        df:pandas.DataFrame, target:pandas.Series, model_names:str,
        optuna_storage:str, optuna_n_trials:int, optuna_study_name_fmt:str
    ):

    outer_fold, inner_folds = load(core.FOLDS)
    outer_fold_funcs, inner_folds_funcs = preprocess_funcs(df, target, outer_fold, inner_folds)

    scores = []
    for i, ((train_idx, test_idx), outer_func) in enumerate(zip(PredefinedSplit(outer_fold).split(), outer_fold_funcs)):

        X = df.iloc[train_idx]
        y = target.iloc[train_idx]

        scores_inner = []
        for j, (inner_fold, funcs) in enumerate(zip(inner_folds[i], inner_folds_funcs)):

            logging.getLogger(__name__).info(f"Hyper param tuning on outer fold {i}, inner fold repeat {j+1}/{len(inner_folds)}")

            fold_objective = partial(objective, inner_fold=inner_fold, funcs=funcs, X=X, y=y, model_names=model_names)

            # select best parameters for model using Optuna
            study = optuna.create_study(study_name=optuna_study_name_fmt % (i, j), direction='minimize', storage=optuna_storage, load_if_exists=True)
            study.set_user_attr("cross-validation splits", core.FOLDS)
            study.optimize(fold_objective, n_trials=optuna_n_trials)

            logging.getLogger(__name__).info(f"Retraining on outer fold {i}, inner fold repeat {j+1}/{len(inner_folds)}")

            # retrain model with best parameters on train+val data
            frozen_trial = study.best_trial
            model = definition_dict[frozen_trial.params["model_name"]](frozen_trial)

            model.fit(outer_func[0](X), outer_func[1](y))

            # predict X_test with retrained model
            X_test = outer_func[0](df.iloc[test_idx])
            y_test_pred = model.predict(X_test)

            # evaluate predictions
            y_test = outer_func[1](target.iloc[test_idx])

            scores_inner.append(sklearn.metrics.mean_squared_error(y_test, y_test_pred))

        scores.append(scores_inner)

    return scores

# Cell

@click.command()
@click.option("--target-col", "-t", type=str, default="IntensityMCTMRlogicle")
@click.option("--storage-path", "-s", type=click.Path(file_okay=False, exists=False), required=True, help="Provide absolute path.")
@click.option("--model", "-m", type=click.Choice(definition_dict.keys()), multiple=True, default=definition_dict.keys())
@click.option("--config", "-c", type=click.Path(dir_okay=False, exists=True), default="../config_cn1346.yml")
@click.option("--optuna-n-trials", "-ot", type=click.IntRange(0), default=5)
@click.option("--optuna-study-name", "-on", type=str, default=5)
@click.option("--overwrite", "-w", is_flag=True, default=False)
@click.option("--verbose", "-v", is_flag=True, default=False)
def nested_cross_validation_with_hyperparam_optimization_command(target_col, storage_path, model, config, optuna_n_trials, optuna_study_name, overwrite, verbose):

    core.load_config(config)

    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)

    meta, df = fcsparser.parse(core.FCS_NONINTENSITY)
    df_meta = df[["label", "replicate", "timepoint"]].astype(int)
    df_meta["timepoint"] = df_meta["timepoint"].map(lambda a: meta["timepoint"].split(",")[a])
    df_meta["replicate"] = df_meta["replicate"].map(lambda a: meta["replicate"].split(",")[a])
    df = df.drop(columns=["label", "replicate", "timepoint"])

    _, fluor_df = fcsparser.parse(core.FCS_FLUOR)

    df = bf_df = df.filter(regex="(i?).*BF.*")
    target = fluor_df[target_col]

    optuna_storage = "sqlite:////"+os.path.join(storage_path, "optuna_db.sqlite")

    if overwrite and os.path.isfile(optuna_storage):
        os.remove(optuna_storage)

    optuna_study_name_fmt = optuna_study_name + "_%d_%d"

    scores = nested_cross_validation_with_hyperparam_optimization(df, target, model, optuna_storage, optuna_n_trials, optuna_study_name_fmt)

    dump(scores, os.path.join(storage_path, "scores.dat"))
