# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/4c_optimal_mrna_content_regression.ipynb (unless otherwise specified).

__all__ = ['standardizer', 'preprocess_funcs', 'define_mlp', 'define_xgb', 'define_sgd', 'definition_dict',
           'nested_cross_validation']

# Cell
# export

import pandas
import os
import numpy
import seaborn
import logging
import matplotlib.pyplot as plt
from importlib import reload

numpy.random.seed(42)

# Cell

import time
import fcsparser
import sklearn.linear_model
import sklearn.model_selection
from calibration_singlecell import classification
from functools import partial
from ehv import mrna_content_regression, core
import scipy.stats
from sklearn.model_selection import PredefinedSplit
from sklearn import metrics
from joblib import load, dump
import sklearn.neural_network
import optuna

reload(core)

# Cell

def standardizer(X, mean, std=1.0):
    return (X-mean)/std

# Cell

def preprocess_funcs(df, target):

    outer_fold_funcs = []
    inner_fold_funcs = []

    outer_fold, inner_folds = load(core.FOLDS)

    for i, (train_idx, _) in enumerate(PredefinedSplit(outer_fold).split()):

        X = df.iloc[train_idx]
        y = target.iloc[train_idx]

        outer_fold_funcs.append((
            partial(standardizer, mean=X.mean(), std=X.std()),
            partial(standardizer, mean=y.mean())
        ))

        for inner_fold in inner_folds[i]:
            tmp = []
            for nested_train_idx, _ in PredefinedSplit(inner_fold).split():

                X_train = X.iloc[nested_train_idx]
                mean, std = X.mean(), X.std()

                y_mean = y.iloc[nested_train_idx].mean()

                tmp.append((
                    partial(standardizer, mean=mean, std=std),
                    partial(standardizer, mean=y_mean)
                ))

            inner_fold_funcs.append(tmp)

    return outer_fold, inner_folds

# Cell

def define_mlp(trial):
    n_layers = trial.suggest_int("n_layers", 2, 10)
    layers = []
    for i in range(n_layers):
        layers.append(trial.suggest_int(f"hidden_layer_size_{i}", 10, 100))

    return sklearn.neural_network.MLPRegressor(
        hidden_layer_sizes = layers,
        activation = trial.suggest_categorical("activation", ['identity', 'logistic', 'tanh', 'relu']),
        learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-1, log=True)
    )


def define_xgb(trial):
    return xgboost.XGBRegressor(
        n_estimators = trial.suggest_int('n_estimators', 50, 400),
        tree_method = "gpu_hist",
        max_depth = trial.suggest_int("max_depth", 3, 20),
        learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True),
        subsample = trial.suggest_float('subsample', 0.1, 1.0)
    )


def define_sgd(trial):
    learning_rate = trial.suggest_categorical("learning_rate", ["constant", "optimal"])
    if learning_rate == "constant":
        eta0 = trial.suggest_float("eta0")
    else:
        eta0 = 0.01 # default

    return sklearn.linear_model.SGDRegressor(
        loss = trial.suggest_categorical("loss", ['squared_loss', 'huber']),
        fit_intercept = False,
        learning_rate = learning_rate,
        eta0 = trial.suggest_float('eta0', 1e-4, 1e-1, log=True),
        alpa = trial.suggest_float("alpha")
    )

definition_dict = {
    "mlp": define_mlp,
    "sgd": define_sgd,
    "xgb": define_xgb
}

# Cell

def nested_cross_validation():

    scores = []
    for i, (outer_func, (train_idx, test_idx)) in enumerate(zip(outer_fold_funcs, PredefinedSplit(outer_fold).split())):

        X = bf_df.iloc[train_idx]
        y = fluor_df[target_col].iloc[train_idx]

        scores_inner = []
        studies_inner = []
        for j, (inner_fold, funcs) in enumerate(zip(inner_folds[i], inner_fold_funcs)):

            logging.getLogger(__name__).info(f"Hyper param tuning on outer fold {i}, inner fold instance {j+1}/{len(inner_folds)}")

            fold_objective = partial(objective, inner_fold=inner_fold, X=X, y=y, model_name_options=definition_dict.keys())

            # select best parameters for model using Optuna
            study = optuna.create_study(direction='minimize', storage="sqlite://test_{i}_{j}.db")
            study.set_user_attr("cross-validation splits", core.FOLDS)
            study.optimize(fold_objective, n_trials=3)

            studies_inner.append(study)

            logging.getLogger(__name__).info(f"Retraining on outer fold {i}, inner fold instance {j+1}/{len(inner_folds)}")

            # retrain model with best parameters on train+val data
            model = sklearn.neural_network.MLPRegressor(
                hidden_layer_sizes=(10, 20),
                activation = study.best_params["activation"],
                learning_rate_init = study.best_params['learning_rate_init']
            )
            model.fit(outer_func[0](X), outer_func[1](y))

            # predict X_test with retrained model
            X_test = outer_func[0](bf_df.iloc[test_idx])
            y_test_pred = model.predict(X_test)

            # evaluate predictions
            y_test = outer_func[1](fluor_df[target_col].iloc[test_idx])

            scores_inner.append(sklearn.metrics.mean_squared_error(y_test, y_test_pred))

        scores.append(scores_inner)
        studies.append(studies_inner)